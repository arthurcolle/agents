# Strategic Hypothesis Canvas

## Overview
This prompt template provides a comprehensive framework for developing and testing critical business assumptions through structured experimentation. By formalizing hypotheses about customers, problems, and solutions, it creates a scientific approach to business strategy validation. The methodology systematically reduces uncertainty by identifying the riskiest assumptions first, designing lean experiments to validate or invalidate them, and generating actionable insights that refine strategic direction with minimal resource investment.

## User Instructions
Provide information about:
1. Your business concept, product idea, or strategic initiative
2. Target customer segments you're considering
3. Key assumptions you've made about the market or opportunity
4. Any evidence or data you already have
5. Resources available for experimentation
6. Timeline constraints for decision-making
7. Key metrics that would indicate success

## System Prompt

I'll develop a comprehensive Strategic Hypothesis Canvas to systematically validate your critical business assumptions through structured experimentation. This rigorous methodology follows a systematic framework:

### 1. Assumption Inventory Development
- Identify all key assumptions underlying your business strategy
- Categorize assumptions by type (customer, problem, solution, revenue, etc.)
- Articulate assumptions in clear, testable statements
- Trace assumptions to their strategic implications
- Identify hidden or implicit assumptions in your thinking
- Prioritize assumptions based on risk and uncertainty
- Map assumption dependencies and relationships

### 2. Hypothesis Formulation Framework
For each critical assumption, craft structured hypotheses with:
- Precise, testable prediction
- Clear conditions for validation/invalidation
- Measurable success criteria
- Explicit timelines
- Specific target segments
- Contextual factors to control

**Format Template:**
"We believe that [specific prediction] for [target segment] because [underlying rationale]. We'll know this is true when we see [measurable outcome] within [timeframe]."

### 3. Risk Assessment Matrix
Evaluate each hypothesis across dimensions:
- **Uncertainty Level**: How confident are we in our current assumption?
- **Strategic Impact**: How critical is this to overall strategy success?
- **Validation Cost**: What resources are required to test this hypothesis?
- **Time Sensitivity**: How quickly must we validate this?
- **Assumption Hierarchy**: What other assumptions depend on this?
- **Market Window**: Will this information still be valuable in the future?
- **Competitive Advantage**: Does validating this create proprietary insight?

### 4. Experiment Design Architecture
For each priority hypothesis, create test protocols for:
- **Minimum Viable Experiment**: Simplest test to provide meaningful data
- **Sample Definition**: Target participants and selection methodology
- **Isolation Strategy**: Controlling variables to ensure clean results
- **Signal Measurement**: Specific data collection methods and metrics
- **Noise Reduction**: Techniques to minimize experimental interference
- **Resource Requirements**: Precise time, cost, and capability needs
- **Success Thresholds**: Quantitative benchmarks for validation

### 5. Evidence Quality Framework
Evaluate data validity using:
- **Statistical Significance**: Mathematical confidence in findings
- **Sample Representativeness**: How well participants match target market
- **Methodological Rigor**: Control for biases and experimental errors
- **Relevance Assessment**: Alignment with original hypothesis
- **Alternative Explanations**: Other interpretations of the evidence
- **Confidence Calibration**: Adjusted certainty based on limitations
- **Replication Potential**: Whether results would be repeatable

### 6. Multi-Method Validation Strategy
Design complementary approaches to strengthen findings:
- **Quantitative Analytics**: Statistical analysis of behavioral data
- **Qualitative Insights**: Interview and observational protocols
- **Prototype Testing**: Limited product simulations
- **Competitive Analysis**: Market-based evidence collection
- **Wizard of Oz Testing**: Human-simulated functionality
- **Concierge Experiments**: Manually delivered solutions
- **Landing Page Tests**: Interest and conversion measurement

### 7. Strategic Insight Integration
Translate experimental results into actionable strategies:
- **Decision Triggers**: Predefined actions based on specific outcomes
- **Pivot Thresholds**: Measurements indicating strategic redirection
- **Scaling Criteria**: Evidence required for resource expansion
- **Learning Documentation**: Structured knowledge capture protocols
- **Assumption Refinement**: Updated belief systems based on findings
- **Strategic Narrative Revision**: Modified value proposition and positioning
- **Resource Reallocation Framework**: Systematic reprioritization approach

### 8. Sequential Experimentation Roadmap
Develop multi-stage validation sequence with:
- **Critical Path Analysis**: Dependency-based testing sequence
- **Parallel Testing Opportunities**: Non-dependent concurrent experiments
- **Milestone Gates**: Decision points requiring validation before proceeding
- **Adaptive Design**: Process for modifying experiments based on early findings
- **Progressive Fidelity**: Evolution from low to high-resolution testing
- **Compounding Insights**: How later tests build on earlier findings
- **Experimentation Velocity**: Cadence and rhythm of testing cycles

### 9. Organizational Learning System
Establish mechanisms to institutionalize insights:
- **Insight Distribution Protocols**: How findings are shared across teams
- **Assumption Registry**: Dynamic database of validated/invalidated beliefs
- **Decision Journal**: Record of choices, rationales, and outcomes
- **Experiment Repository**: Archive of methodologies and results
- **Prediction Market**: Internal system for forecasting hypothesis outcomes
- **Post-Mortem Framework**: Structured analysis of experimental failures
- **Knowledge Management System**: Long-term insight preservation

### 10. Strategic Adaptability Framework
Create systematic approach to hypothesis-driven pivots:
- **Signal Detection**: Early indicators of validation/invalidation
- **Strategic Option Mapping**: Alternative paths based on results
- **Incremental Adaptation Model**: Continuous strategy refinement approach
- **Discontinuous Pivot Protocol**: Process for major directional changes
- **Investment Staging**: Resource allocation tied to validation milestones
- **Commitment Management**: Balancing conviction with empirical flexibility
- **Market Timing Calibration**: Adjusting velocity based on opportunity windows

The completed Strategic Hypothesis Canvas will provide a comprehensive framework for systematically validating your critical business assumptions, with clear experiments, decision criteria, and a roadmap for evolving your strategy based on empirical evidence.

## Example Usage

### User Input
Business concept: Subscription-based platform offering AI-powered legal document review for small businesses
Target customers: Solo lawyers, small law firms (2-5 attorneys), small businesses without in-house counsel
Key assumptions: Small firms struggle with document review efficiency; AI can reduce review time by 60%; target willingness to pay $300/month
Current evidence: Interviewed 5 small firm lawyers who expressed interest; competitive solutions cost $500+ monthly
Resources: $30K for validation; technical co-founder for prototype; 3 months before funding decision
Success metrics: 20+ pre-orders; demonstrated 40%+ time savings; clear product requirements

### System Response

# Strategic Hypothesis Canvas: AI-Powered Legal Document Review Platform

## 1. Assumption Inventory

**Customer Assumptions:**
1. Solo lawyers and small firms (2-5 attorneys) struggle with document review efficiency
2. These firms lack resources for comprehensive technology solutions
3. Decision-makers prioritize time savings over perfect accuracy
4. Small businesses without in-house counsel regularly need legal document review
5. Target customers have sufficient technical literacy to adopt AI solutions
6. Firms are willing to try new technologies in their legal practice
7. Document review is a significant pain point in their workflow

**Problem Assumptions:**
1. Document review consumes 30%+ of lawyers' billable time
2. Current solutions are too expensive for small firms ($500+ monthly)
3. Manual review creates bottlenecks in client service delivery
4. Accuracy concerns prevent delegation to non-attorney staff
5. Volume of documents needing review is significant and consistent
6. Existing document review practices create substantial error risk
7. The inefficiency problem is valuable enough to solve ($300/mo value)

**Solution Assumptions:**
1. AI can reduce document review time by 60%
2. Small firms will trust AI-assisted document review
3. Our technical approach will work across diverse document types
4. Users can easily understand and validate AI recommendations
5. Implementation requires minimal change to existing workflows
6. Solutions can be deployed without significant IT support
7. Service can operate with acceptable accuracy at launch

**Business Model Assumptions:**
1. Target customers will pay $300/month for the solution
2. Subscription model aligns with customer purchasing preferences
3. Customer acquisition costs will be under $1,000 per customer
4. Churn rate will be under 5% monthly after onboarding
5. Word-of-mouth will drive significant organic acquisition
6. Regulatory requirements won't create prohibitive compliance costs
7. Platform can scale effectively with minimal marginal costs

## 2. Prioritized Hypotheses

After risk assessment, these represent your most critical assumptions requiring validation:

**Hypothesis 1: Problem Validation**
"We believe that small law firms (2-5 attorneys) spend at least 30% of their billable time on document review and consider this a significant workflow bottleneck because it prevents focusing on higher-value activities. We'll know this is true when we see at least 75% of interviewed firms reporting this level of time allocation and identifying it as a top-3 pain point within 3 weeks of research."

**Hypothesis 2: Value Proposition Validation**
"We believe that small legal practices would adopt an AI-powered document review solution that reduces review time by 40% or more because time savings directly impacts their profitability and client service capacity. We'll know this is true when we see at least 65% of demonstration participants expressing strong interest (8+ on a 10-point scale) and at least 20% committing to pre-orders within 2 weeks of demonstration."

**Hypothesis 3: Pricing Validation**
"We believe that small law firms are willing to pay $300/month for an AI-powered document review solution because this represents less than the cost of 3 billable hours while potentially saving 20+ hours monthly. We'll know this is true when we see at least 40% of interested prospects accepting this price point without negotiation and at least 15 pre-orders at this price within 4 weeks of offer presentation."

**Hypothesis 4: Technical Feasibility**
"We believe that our AI approach can achieve at least 85% accuracy across standard legal documents (contracts, briefs, corporate filings) because our algorithmic approach is designed for legal domain specificity. We'll know this is true when our prototype correctly identifies at least 85% of key provisions and potential issues across a test set of 50 diverse documents within 6 weeks of development."

**Hypothesis 5: Trust Validation**
"We believe that attorneys will trust AI-assisted document review for routine matters when they can easily verify and override suggestions because this provides efficiency while maintaining professional control. We'll know this is true when at least 70% of test users express comfort using the system for client work after a supervised trial period of 2 weeks."

## 3. Experimental Design Plan

### Experiment 1: Problem Validation Research
**Type:** Mixed-method research study
**Hypothesis Tested:** #1 (Time spent on document review)
**Design:**
- Structured interviews with 20 attorneys from small firms (2-5 lawyers)
- Time allocation survey distributed to 100+ small firm attorneys
- Analysis of 3-5 workflow shadow observations in target firms
- Document review volume and frequency assessment

**Success Metrics:**
- % of firms reporting 30%+ time on document review
- Ranking of document review among practice pain points
- Observed vs. reported time allocation discrepancies
- Emotional intensity when discussing document review challenges

**Resource Requirements:**
- 40 hours research time
- $1,000 participant incentives
- 3 weeks timeline
- Research discussion guide and survey instruments

### Experiment 2: Prototype Demonstration Test
**Type:** Solution concept validation
**Hypotheses Tested:** #2 (Value proposition) and #5 (Trust validation)
**Design:**
- Interactive prototype demonstrating core functionality
- Supervised testing sessions with 15 target users
- Simulated document review tasks with/without solution
- Pre/post testing attitude measurement
- Structured evaluation of perceived value and trust barriers

**Success Metrics:**
- Measured time savings in controlled review tasks
- Interest level ratings (1-10 scale)
- Trust comfort assessment (qualitative and quantitative)
- Willingness to try in actual client work
- Feature prioritization insights

**Resource Requirements:**
- 80 hours technical development (prototype)
- 30 hours testing facilitation
- $2,000 participant incentives
- 5 weeks timeline (development and testing)
- Clickable prototype with simulated AI functionality

### Experiment 3: Pricing Sensitivity Analysis
**Type:** Quantitative market testing
**Hypothesis Tested:** #3 (Pricing validation)
**Design:**
- Van Westendorp price sensitivity survey with 50+ target customers
- Tiered subscription offer testing (3 price points)
- Competitor price comparison perception study
- Value-anchoring experiment with ROI calculation tool
- Pre-order campaign with 3 commitment levels

**Success Metrics:**
- Acceptance rate at $300/month price point
- Price elasticity across different segments
- Pre-order conversion rate at target price
- ROI perception based on presented value model
- Competitive price positioning perception

**Resource Requirements:**
- 30 hours research design and analysis
- $1,500 survey incentives
- 4 weeks timeline
- Pricing survey instrument
- Pre-order landing page development

### Experiment 4: Technical Accuracy Assessment
**Type:** Technical validation
**Hypothesis Tested:** #4 (Technical feasibility)
**Design:**
- Development of core algorithm prototype
- Testing against library of 50 diverse legal documents
- Blind comparison against human review (benchmark)
- Error type and frequency analysis
- Improvement iteration cycle with ML feedback loop

**Success Metrics:**
- Overall accuracy rate against human review
- Performance across different document types
- Error pattern identification and classification
- Improvement rate through iteration
- Processing time per document analysis

**Resource Requirements:**
- 120 hours technical development
- 40 hours test document preparation and analysis
- $0 additional costs (using existing documents)
- 6 weeks timeline
- Document test corpus development

### Experiment 5: Pilot User Program
**Type:** Limited release validation
**Hypotheses Tested:** Multiple (integrated solution validation)
**Design:**
- 5-8 pilot customers using actual solution for 2 weeks
- Daily usage tracking and satisfaction measurement
- Weekly feedback interviews and usage pattern analysis
- Problem/solution fit assessment in real conditions
- Iterative enhancements based on usage patterns

**Success Metrics:**
- Actual usage frequency and depth
- Reported time savings in natural environment
- Feature utilization patterns
- Satisfaction trend over usage period
- Willingness to continue post-pilot (conversion)

**Resource Requirements:**
- 100 hours technical support and iteration
- 40 hours customer success management
- $0 additional costs (value-based arrangement)
- 4 weeks timeline (from technical validation)
- Pilot program operation protocols

## 4. Sequenced Validation Roadmap

**Phase 1: Problem Validation (Weeks 1-3)**
- Execute Experiment 1: Problem Validation Research
- Deliverable: Comprehensive problem validation report
- Go/No-Go Decision: Proceed if 75%+ of firms report significant document review time allocation
- Resource Commitment: $1,000 + 40 hours

**Phase 2: Solution Concept & Technical Foundation (Weeks 4-9)**
- Execute Experiment 4: Technical Accuracy Assessment (Weeks 4-9)
- Execute Experiment 2: Prototype Demonstration Test (Weeks 6-9)
- Deliverables: Working algorithm prototype + Solution concept validation
- Go/No-Go Decision: Proceed if technical accuracy exceeds 85% AND interest rating averages 8+
- Resource Commitment: $2,000 + 270 hours

**Phase 3: Business Model Validation (Weeks 10-12)**
- Execute Experiment 3: Pricing Sensitivity Analysis
- Deliverable: Pricing strategy and initial pre-orders
- Go/No-Go Decision: Proceed if 40%+ accept target pricing AND pre-orders exceed 15
- Resource Commitment: $1,500 + 30 hours

**Phase 4: Integrated Solution Validation (Weeks 13-16)**
- Execute Experiment 5: Pilot User Program
- Deliverable: Validated solution with customer success evidence
- Final Go/No-Go Decision: Proceed to funding round if pilot users report 40%+ time savings AND 80%+ indicate continued usage intent
- Resource Commitment: 140 hours

## 5. Risk Management Framework

**Critical Risks and Contingencies:**

**Risk 1: Insufficient Problem Severity**
- Early Detection: <50% of firms report significant document review burden
- Contingency: Pivot to investigate alternative pain points in legal workflow
- Decision Trigger: Week 3 problem validation results

**Risk 2: Technical Performance Limitations**
- Early Detection: Accuracy <70% after first algorithm iteration
- Contingency: Narrow document type scope to achieve higher accuracy in specific domains
- Decision Trigger: Week 7 technical accuracy assessment

**Risk 3: Trust Barrier Too High**
- Early Detection: <50% comfort level with AI recommendations
- Contingency: Redesign interface to emphasize human oversight and control
- Decision Trigger: Week 9 prototype demonstration results

**Risk 4: Price Resistance**
- Early Detection: <25% acceptance of target price point
- Contingency: Develop tiered offering with entry-level option at lower price point
- Decision Trigger: Week 11 pricing analysis results

**Risk 5: Weak Actual Usage Patterns**
- Early Detection: <3 active uses per week during pilot
- Contingency: Implement workflow integration enhancements and usage incentives
- Decision Trigger: Week 14 pilot usage data

## 6. Learning Documentation System

For each experiment, structured knowledge capture will include:

**Validated Learning Register:**
- Original hypothesis statement
- Experimental methods employed
- Key quantitative findings
- Critical qualitative insights
- Degree of hypothesis validation/invalidation
- Surprising or unexpected observations
- Refined assumption based on evidence
- Strategic implications of findings
- Follow-up questions generated

**Decision Journal Entries:**
- Context and constraints at decision point
- Options considered and evaluation criteria
- Selected direction and justification
- Anticipated outcomes and risks
- Actual results compared to expectations
- Reflective analysis on decision quality

## 7. Strategic Adaptation Framework

**Strategic Pivot Triggers:**
- Problem Pivot: <60% of targets identify document review as significant pain point
- Customer Pivot: >80% interest from adjacent segment (e.g., in-house counsel)
- Solution Pivot: <25% perceived value-add from AI components vs. workflow organization
- Channel Pivot: >3x response rate from alternative acquisition channel
- Revenue Pivot: <10 pre-orders at target price point
- Technology Pivot: <75% accuracy on general documents but >90% on specific document type

**Scaling Triggers:**
- Proceed to seed funding when: 20+ pre-orders secured AND pilot results show 40%+ time savings
- Expand marketing when: Customer acquisition cost <$500 with positive 6-month customer value
- Increase feature development when: Core solution achieves >90% satisfaction rating
- Explore enterprise segment when: 3+ unsolicited inquiries from larger organizations

This Strategic Hypothesis Canvas provides a comprehensive framework for systematically validating your AI-powered legal document review platform concept, with clearly defined experiments, decision criteria, and a roadmap for evolving your strategy based on empirical evidence rather than assumptions.